{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_limit, action_space, device):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "        self.action_space = action_space\n",
    "        self.device = device\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        state_lst, reward_lst, next_state_lst, done_mask_lst = [], [], [], []\n",
    "        actions_lst = [[] for _ in range(self.action_space)]\n",
    "\n",
    "        for state, actions, reward, next_state, done_mask in mini_batch:\n",
    "            state_lst.append(state)\n",
    "            for i in range(self.action_space):\n",
    "                actions_lst[i].append(actions[i])\n",
    "            reward_lst.append([reward])\n",
    "            next_state_lst.append(next_state)\n",
    "            done_mask_lst.append([done_mask])\n",
    "\n",
    "        states = torch.tensor(np.array(state_lst, dtype=np.float32), device=self.device)\n",
    "        rewards = torch.tensor(np.array(reward_lst, dtype=np.float32), device=self.device)\n",
    "        next_states = torch.tensor(np.array(next_state_lst, dtype=np.float32), device=self.device)\n",
    "        done_masks = torch.tensor(np.array(done_mask_lst, dtype=np.float32), device=self.device)\n",
    "        actions = [torch.tensor(x, dtype=torch.float32, device=self.device) for x in actions_lst]\n",
    "\n",
    "        return states, actions, rewards, next_states, done_masks\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_space: int, action_num: int, action_scale: int):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(state_space, state_space * 20)\n",
    "        self.linear_2 = nn.Linear(state_space * 20, state_space * 10)\n",
    "\n",
    "        self.actions = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(state_space * 10, state_space * 5),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(state_space * 5, action_scale)\n",
    "            ) for _ in range(action_num)\n",
    "        ])\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(state_space * 10, state_space * 5),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(state_space * 5, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear_1(x))\n",
    "        encoded = F.relu(self.linear_2(x))\n",
    "        actions = [head(encoded) for head in self.actions]\n",
    "        value = self.value(encoded)\n",
    "\n",
    "        for i in range(len(actions)):\n",
    "            actions[i] = actions[i] - actions[i].max(dim=-1, keepdim=True)[0]\n",
    "            actions[i] += value\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BQN(nn.Module):\n",
    "    def __init__(self, state_space: int, action_num: int, action_scale: int,\n",
    "                 learning_rate: float, device: str):\n",
    "        super().__init__()\n",
    "        self.q = QNetwork(state_space, action_num, action_scale).to(device)\n",
    "        self.target_q = QNetwork(state_space, action_num, action_scale).to(device)\n",
    "        self.target_q.load_state_dict(self.q.state_dict())\n",
    "\n",
    "        self.optimizer = optim.Adam([\n",
    "            {'params': self.q.linear_1.parameters(), 'lr': learning_rate / (action_num + 2)},\n",
    "            {'params': self.q.linear_2.parameters(), 'lr': learning_rate / (action_num + 2)},\n",
    "            {'params': self.q.value.parameters(), 'lr': learning_rate / (action_num + 2)},\n",
    "            {'params': self.q.actions.parameters(), 'lr': learning_rate},\n",
    "        ])\n",
    "\n",
    "        self.device = device\n",
    "        self.update_freq = 1000\n",
    "        self.update_count = 0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def action(self, x):\n",
    "        return self.q(x)\n",
    "\n",
    "    def train_mode(self, n_epi, memory, batch_size, gamma, use_tensorboard=False, writer=None):\n",
    "        states, actions, rewards, next_states, done_masks = memory.sample(batch_size)\n",
    "        actions = torch.stack(actions).transpose(0, 1).unsqueeze(-1)  \n",
    "        done_masks = 1 - done_masks\n",
    "\n",
    "        cur_q = self.q(states)\n",
    "        cur_q = torch.stack(cur_q).transpose(0, 1)\n",
    "        cur_q_selected = cur_q.gather(2, actions.long()).squeeze(-1)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q = self.target_q(next_states)\n",
    "            target_q = torch.stack(target_q).transpose(0, 1)\n",
    "            target_q_max = target_q.max(dim=-1, keepdim=True)[0].mean(dim=1)\n",
    "            target_val = rewards + gamma * done_masks * target_q_max\n",
    "\n",
    "        loss = F.mse_loss(cur_q_selected, target_val.repeat(1, cur_q_selected.shape[1]))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.update_freq == 0:\n",
    "            self.target_q.load_state_dict(self.q.state_dict())\n",
    "            self.update_count = 0\n",
    "\n",
    "        if use_tensorboard and writer is not None:\n",
    "            writer.add_scalar(\"Loss/loss\", loss.item(), n_epi)\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = True\n",
    "RENDER = False\n",
    "EPOCHS = 1000\n",
    "USE_TENSORBOARD = True\n",
    "LEARNING_RATE = 1e-4\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "ACTION_SCALE = 6\n",
    "SAVE_INTERVAL = 100\n",
    "PRINT_INTERVAL = 1\n",
    "LOAD_MODEL = 'no'\n",
    "\n",
    "writer = SummaryWriter() if USE_TENSORBOARD else None\n",
    "os.makedirs('./model_weights', exist_ok=True)\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"human\" if RENDER else \"rgb_array\")\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_space = env.action_space.shape[0]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "agent = BQN(state_space, action_space, ACTION_SCALE, LEARNING_RATE, device).to(device)\n",
    "if LOAD_MODEL != 'no':\n",
    "    agent.load_state_dict(torch.load(f'./model_weights/{LOAD_MODEL}', map_location=device))\n",
    "\n",
    "memory = ReplayBuffer(buffer_limit=100000, action_space=action_space, device=device)\n",
    "real_action = np.linspace(-1., 1., ACTION_SCALE)\n",
    "\n",
    "scores, avg_scores = [], []\n",
    "\n",
    "for n_epi in range(1, EPOCHS + 1):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    score = 0.0\n",
    "    epsilon = max(0.01, 0.9 - 0.01 * (n_epi / 10))\n",
    "\n",
    "    while not done:\n",
    "        if RENDER:\n",
    "            env.render()\n",
    "\n",
    "        if random.random() < epsilon:\n",
    "            action_indices = random.sample(range(ACTION_SCALE), action_space)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "                action_prob = agent.action(state_tensor)\n",
    "                action_indices = [p.argmax(dim=1).item() for p in action_prob]\n",
    "\n",
    "        selected_action = np.array([real_action[a] for a in action_indices])\n",
    "        next_state, reward, terminated, truncated, _ = env.step(selected_action)\n",
    "        done = terminated or truncated\n",
    "        memory.put((state, action_indices, reward, next_state, float(done)))\n",
    "        state = next_state\n",
    "        score += reward\n",
    "\n",
    "        if memory.size() > 5000 and TRAIN:\n",
    "            agent.train_mode(n_epi, memory, BATCH_SIZE, GAMMA, USE_TENSORBOARD, writer)\n",
    "\n",
    "    scores.append(score)\n",
    "    avg_score = np.mean(scores[-100:])\n",
    "    avg_scores.append(avg_score)\n",
    "\n",
    "    if USE_TENSORBOARD and writer is not None:\n",
    "        writer.add_scalar(\"Reward/score\", score, n_epi)\n",
    "        writer.add_scalar(\"Reward/avg_score\", avg_score, n_epi)\n",
    "\n",
    "    if n_epi % SAVE_INTERVAL == 0:\n",
    "        torch.save(agent.state_dict(), f'./model_weights/agent_{n_epi}.pth')\n",
    "\n",
    "    if n_epi % PRINT_INTERVAL == 0:\n",
    "        print(f\"Episode: {n_epi}, Score: {score:.2f}, Avg Score: {avg_score:.2f}, Epsilon: {epsilon:.2f}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(scores, label='Score per Episode')\n",
    "plt.plot(avg_scores, label='Average Score (last 100 episodes)')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "env.close()\n",
    "\n",
    "env = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
