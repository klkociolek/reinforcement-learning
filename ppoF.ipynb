{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-4\n",
    "MAX_GRAD_NORM = 0.5\n",
    "ENT_WEIGHT = 0.01 \n",
    "CLIP_VAL = 0.2\n",
    "N_EPOCHS = 4\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99  \n",
    "LAMBDA = 0.95  \n",
    "MAX_STEPS = 2048\n",
    "MAX_EPISODES = 1500\n",
    "SAVE_INTERVAL = 200 \n",
    "SAVE_DIR = \"save\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Used devic: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedNormal(torch.distributions.Normal):\n",
    "    def log_probs(self, x):\n",
    "        return super().log_prob(x).sum(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        return super().entropy().sum(-1)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddBias(nn.Module):\n",
    "    def __init__(self, bias):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(bias.unsqueeze(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = self.bias.t().view(1, -1)\n",
    "        return x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiagGaussian(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.fc_mean = nn.Linear(inp_dim, out_dim)\n",
    "        self.log_std = AddBias(torch.zeros(out_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = self.fc_mean(x)\n",
    "        logstd = self.log_std(torch.zeros_like(mean))\n",
    "        return FixedNormal(mean, logstd.exp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, s_dim, a_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(s_dim, 128), nn.ReLU(), nn.Linear(128, 128), nn.ReLU()\n",
    "        )\n",
    "        self.dist = DiagGaussian(128, a_dim)\n",
    "\n",
    "    def forward(self, state, deterministic=False):\n",
    "        features = self.main(state)\n",
    "        dist = self.dist(features)\n",
    "        action = dist.mode() if deterministic else dist.sample()\n",
    "        return action, dist.log_probs(action)\n",
    "\n",
    "    def choose_action(self, state, deterministic=False):\n",
    "        with torch.no_grad():\n",
    "            action, _ = self.forward(state, deterministic)\n",
    "        return action\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        features = self.main(state)\n",
    "        dist = self.dist(features)\n",
    "        return dist.log_probs(action), dist.entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, s_dim):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(s_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.main(state).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvRunner:\n",
    "    def __init__(self, s_dim, a_dim, max_step=MAX_STEPS, gamma=GAMMA, device=device):\n",
    "        self.s_dim = s_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.max_step = max_step\n",
    "        self.gamma = gamma\n",
    "        self.device = device\n",
    "\n",
    "        # Prepare buffers for a single rollout\n",
    "        self.mb_states = np.zeros((max_step, s_dim), dtype=np.float32)\n",
    "        self.mb_actions = np.zeros((max_step, a_dim), dtype=np.float32)\n",
    "        self.mb_values = np.zeros((max_step,), dtype=np.float32)\n",
    "        self.mb_rewards = np.zeros((max_step,), dtype=np.float32)\n",
    "        self.mb_a_logps = np.zeros((max_step,), dtype=np.float32)\n",
    "\n",
    "    def compute_discounted_return(self, rewards, last_value):\n",
    "        returns = np.zeros_like(rewards)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                returns[t] = rewards[t] + self.gamma * last_value\n",
    "            else:\n",
    "                returns[t] = rewards[t] + self.gamma * returns[t + 1]\n",
    "        return returns\n",
    "\n",
    "    def run(self, env, policy_net, value_net):\n",
    "        state, _ = env.reset()\n",
    "        episode_len = self.max_step\n",
    "\n",
    "        for step in range(self.max_step):\n",
    "            state_t = torch.tensor(state[None], dtype=torch.float32, device=self.device)\n",
    "            action, a_logp = policy_net(state_t)\n",
    "            value = value_net(state_t)\n",
    "\n",
    "            # Move data back to CPU numpy\n",
    "            action_np = action.cpu().numpy()[0]\n",
    "            self.mb_states[step] = state\n",
    "            self.mb_actions[step] = action_np\n",
    "            self.mb_a_logps[step] = a_logp.cpu().detach().numpy()\n",
    "            self.mb_values[step] = value.cpu().detach().numpy()\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action_np)\n",
    "            done = terminated or truncated\n",
    "            self.mb_rewards[step] = reward\n",
    "\n",
    "            if done:\n",
    "                episode_len = step + 1\n",
    "                break\n",
    "            state = next_state\n",
    "\n",
    "        # Compute returns using the last state value\n",
    "        last_value = (\n",
    "            value_net(\n",
    "                torch.tensor(next_state[None], dtype=torch.float32, device=self.device)\n",
    "            )\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "            if done is False\n",
    "            else np.array([0.0])\n",
    "        )\n",
    "\n",
    "        mb_returns = self.compute_discounted_return(\n",
    "            self.mb_rewards[:episode_len], last_value\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.mb_states[:episode_len],\n",
    "            self.mb_actions[:episode_len],\n",
    "            self.mb_a_logps[:episode_len],\n",
    "            self.mb_values[:episode_len],\n",
    "            mb_returns,\n",
    "            self.mb_rewards[:episode_len],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(\n",
    "        self,\n",
    "        policy_net,\n",
    "        value_net,\n",
    "        lr=LR,\n",
    "        max_grad_norm=MAX_GRAD_NORM,\n",
    "        ent_weight=ENT_WEIGHT,\n",
    "        clip_val=CLIP_VAL,\n",
    "        sample_n_epoch=N_EPOCHS,\n",
    "        sample_mb_size=BATCH_SIZE,\n",
    "        device=device,\n",
    "    ):\n",
    "        self.policy_net = policy_net\n",
    "        self.value_net = value_net\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.ent_weight = ent_weight\n",
    "        self.clip_val = clip_val\n",
    "        self.sample_n_epoch = sample_n_epoch\n",
    "        self.sample_mb_size = sample_mb_size\n",
    "        self.device = device\n",
    "\n",
    "        self.opt_policy = torch.optim.Adam(policy_net.parameters(), lr)\n",
    "        self.opt_value = torch.optim.Adam(value_net.parameters(), lr)\n",
    "\n",
    "    def train(\n",
    "        self, mb_states, mb_actions, mb_old_values, mb_advs, mb_returns, mb_old_a_logps\n",
    "    ):\n",
    "        mb_states_t = torch.tensor(mb_states, dtype=torch.float32, device=self.device)\n",
    "        mb_actions_t = torch.tensor(mb_actions, dtype=torch.float32, device=self.device)\n",
    "        mb_old_values_t = torch.tensor(\n",
    "            mb_old_values, dtype=torch.float32, device=self.device\n",
    "        )\n",
    "        mb_advs_t = torch.tensor(mb_advs, dtype=torch.float32, device=self.device)\n",
    "        mb_returns_t = torch.tensor(mb_returns, dtype=torch.float32, device=self.device)\n",
    "        mb_old_a_logps_t = torch.tensor(\n",
    "            mb_old_a_logps, dtype=torch.float32, device=self.device\n",
    "        )\n",
    "\n",
    "        episode_length = len(mb_states_t)\n",
    "        indices = np.arange(episode_length)\n",
    "        mini_batch_count = max(1, episode_length // self.sample_mb_size)\n",
    "\n",
    "        for _ in range(self.sample_n_epoch):\n",
    "            np.random.shuffle(indices)\n",
    "            for i in range(mini_batch_count):\n",
    "                batch_indices = indices[\n",
    "                    i * self.sample_mb_size : (i + 1) * self.sample_mb_size\n",
    "                ]\n",
    "                s_batch = mb_states_t[batch_indices]\n",
    "                a_batch = mb_actions_t[batch_indices]\n",
    "                old_v_batch = mb_old_values_t[batch_indices]\n",
    "                adv_batch = mb_advs_t[batch_indices]\n",
    "                ret_batch = mb_returns_t[batch_indices]\n",
    "                old_logp_batch = mb_old_a_logps_t[batch_indices]\n",
    "\n",
    "                # Evaluate current policy on the mini-batch\n",
    "                curr_logp, entropy = self.policy_net.evaluate(s_batch, a_batch)\n",
    "                curr_values = self.value_net(s_batch)\n",
    "\n",
    "                # Value loss\n",
    "                v_clipped = old_v_batch + torch.clamp(\n",
    "                    curr_values - old_v_batch, -self.clip_val, self.clip_val\n",
    "                )\n",
    "                v_loss_1 = (ret_batch - curr_values).pow(2)\n",
    "                v_loss_2 = (ret_batch - v_clipped).pow(2)\n",
    "                value_loss = torch.max(v_loss_1, v_loss_2).mean()\n",
    "\n",
    "                # Policy loss\n",
    "                ratio = torch.exp(curr_logp - old_logp_batch)\n",
    "                pg_loss_1 = -adv_batch * ratio\n",
    "                pg_loss_2 = -adv_batch * torch.clamp(\n",
    "                    ratio, 1.0 - self.clip_val, 1.0 + self.clip_val\n",
    "                )\n",
    "                policy_loss = torch.max(pg_loss_1, pg_loss_2).mean()\n",
    "                policy_loss -= self.ent_weight * entropy.mean()\n",
    "\n",
    "                # Update policy\n",
    "                self.opt_policy.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    self.policy_net.parameters(), self.max_grad_norm\n",
    "                )\n",
    "                self.opt_policy.step()\n",
    "\n",
    "                # Update value\n",
    "                self.opt_value.zero_grad()\n",
    "                value_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(\n",
    "                    self.value_net.parameters(), self.max_grad_norm\n",
    "                )\n",
    "                self.opt_value.step()\n",
    "\n",
    "        return policy_loss.item(), value_loss.item(), entropy.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(policy_net, env, device=device):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while True:\n",
    "        env.render()\n",
    "        st = torch.tensor(state[None], dtype=torch.float32, device=device)\n",
    "        action = policy_net.choose_action(st, deterministic=True).cpu().numpy()\n",
    "\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action[0])\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "\n",
    "        if terminated or truncated:\n",
    "            print(f\"[Evaluation] Total reward: {total_reward:.2f}, steps: {steps}\")\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(\n",
    "    env_train, policy_net, value_net, runner, agent, max_episodes=MAX_EPISODES\n",
    "):\n",
    "    rewards_history = []\n",
    "    lengths_history = []\n",
    "\n",
    "    sum_reward, sum_length = 0.0, 0\n",
    "\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    for ep in range(1, max_episodes + 1):\n",
    "        with torch.no_grad():\n",
    "            s_mb, a_mb, old_logp_mb, v_mb, ret_mb, r_mb = runner.run(\n",
    "                env_train, policy_net, value_net\n",
    "            )\n",
    "\n",
    "            adv_mb = ret_mb - v_mb\n",
    "            adv_mb = (adv_mb - adv_mb.mean()) / (adv_mb.std() + 1e-8)\n",
    "\n",
    "        pg_loss, v_loss, entropy = agent.train(\n",
    "            s_mb, a_mb, v_mb, adv_mb, ret_mb, old_logp_mb\n",
    "        )\n",
    "\n",
    "        episode_reward = r_mb.sum()\n",
    "        episode_length = len(s_mb)\n",
    "        rewards_history.append(episode_reward)\n",
    "        lengths_history.append(episode_length)\n",
    "        sum_reward += episode_reward\n",
    "        sum_length += episode_length\n",
    "\n",
    "        print(\n",
    "            f\"[Episode {ep:4d}] Reward: {episode_reward:.2f}  Steps: {episode_length}\"\n",
    "        )\n",
    "\n",
    "        if ep % SAVE_INTERVAL == 0:\n",
    "            avg_reward = sum_reward / SAVE_INTERVAL\n",
    "            avg_length = sum_length / SAVE_INTERVAL\n",
    "            print(f\"\\nEpisode {ep}/{max_episodes} - Saving model...\")\n",
    "            print(\"----------------------------------\")\n",
    "            print(f\"Actor Loss   = {pg_loss:.6f}\")\n",
    "            print(f\"Critic Loss  = {v_loss:.6f}\")\n",
    "            print(f\"Entropy      = {entropy:.6f}\")\n",
    "            print(f\"Avg Reward   = {avg_reward:.2f}\")\n",
    "            print(f\"Avg Length   = {avg_length:.2f}\")\n",
    "\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"episode\": ep,\n",
    "                    \"policy_net\": policy_net.state_dict(),\n",
    "                    \"value_net\": value_net.state_dict(),\n",
    "                },\n",
    "                os.path.join(SAVE_DIR, \"model.pt\"),\n",
    "            )\n",
    "\n",
    "            sum_reward, sum_length = 0.0, 0\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards_history)\n",
    "    plt.title(\"Total Reward per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(lengths_history)\n",
    "    plt.title(\"Episode Length per Episode\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Length\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"leaving train agent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_train = gym.make(\"BipedalWalker-v3\", render_mode=\"rgb_array\")\n",
    "s_dim = env_train.observation_space.shape[0]\n",
    "a_dim = env_train.action_space.shape[0]\n",
    "print(f\"State Dimension: {s_dim}, Action Dimension: {a_dim}\")\n",
    "\n",
    "policy = PolicyNet(s_dim, a_dim).to(device)\n",
    "value = ValueNet(s_dim).to(device)\n",
    "runner = EnvRunner(s_dim, a_dim, max_step=MAX_STEPS, gamma=GAMMA, device=device)\n",
    "agent = PPO(\n",
    "    policy,\n",
    "    value,\n",
    "    lr=LR,\n",
    "    max_grad_norm=MAX_GRAD_NORM,\n",
    "    ent_weight=ENT_WEIGHT,\n",
    "    clip_val=CLIP_VAL,\n",
    "    sample_n_epoch=N_EPOCHS,\n",
    "    sample_mb_size=BATCH_SIZE,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"\\nEvaluating the untrained agent:\")\n",
    "_ = runner.run(env_train, policy, value) \n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "train_agent(env_train, policy, value, runner, agent, max_episodes=MAX_EPISODES)\n",
    "print(\"\\nTraining complete\")\n",
    "\n",
    "env_eval = gym.make(\"BipedalWalker-v3\", render_mode=\"human\")\n",
    "print(\"\\nFinal evaluation with rendering:\")\n",
    "play(policy, env_eval, device=device)\n",
    "\n",
    "env_train.close()\n",
    "env_eval.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
